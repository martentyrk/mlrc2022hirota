{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZjARsaJiaYE"
      },
      "source": [
        "# Integrated gradients and attribution score calculations for LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUhHGfUnifxL"
      },
      "source": [
        "This notebook contains the used code for computing the attribution scores with the integrated gradients method for the LSTM classifier. The following models can be analysed for LSTM: \"human caption\" model, nic and nic+equalizer. For these models, we used the same test sets (image id's) where all models were trained on seed 0. The prediction files (equal to the test set) and weights for the LSTM models can be in the folder bias_data_for_ig. The notebook is currently set up to run the NIC model.\n",
        "\n",
        "Credits to Ruben Winastwan for providing a tutorial on how to use Captum for BERT Models. https://towardsdatascience.com/interpreting-the-prediction-of-bert-model-for-text-classification-5ab09f8ef074"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDPrfsMB8S0c"
      },
      "source": [
        "# IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPJNifyL7qQL"
      },
      "outputs": [],
      "source": [
        "import captum\n",
        "\n",
        "import spacy\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.vocab import Vocab\n",
        "\n",
        "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
        "import csv\n",
        "import re\n",
        "import pickle\n",
        "import random\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "import pprint\n",
        "from nltk.tokenize import word_tokenize\n",
        "from io import open\n",
        "import sys\n",
        "import json\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm, trange\n",
        "from operator import itemgetter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjRLSv5C8gog"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1zD8vs8H8Wgg"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        embedded = self.dropout(self.embedding(text)) #[sent len, batch size, emb dim]\n",
        "        \n",
        "        \n",
        "        # pack sequence\n",
        "        # lengths need to be on CPU!\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        #unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        #and apply dropout\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "            \n",
        "        return self.fc(hidden), embedded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohynP_ojADXN"
      },
      "source": [
        "# CAPTION MODEL DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdDXr7x990Di"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "TEXT = data.Field(sequential=True, \n",
        "                tokenize='spacy', \n",
        "                tokenizer_language='en_core_web_sm',\n",
        "                include_lengths=True, \n",
        "                use_vocab=True)\n",
        "LABEL = data.Field(sequential=False, \n",
        "                  use_vocab=False, \n",
        "                  pad_token=None, \n",
        "                  unk_token=None,\n",
        "                  )\n",
        "IMID = data.Field(sequential=False,\n",
        "                  use_vocab=False,\n",
        "                  pad_token=None,\n",
        "                  unk_token=None,\n",
        "                  )\n",
        "train_val_fields = [\n",
        "    ('prediction', TEXT), # process it as text\n",
        "    ('label', LABEL), # process it as label\n",
        "    ('imid', IMID)\n",
        "]\n",
        "\n",
        "train_data, test_data = torchtext.legacy.data.TabularDataset.splits(path='/bias_data_for_ig/LSTM/nic/',train='train_nic_model.csv', test='test_nic_model.csv',\n",
        "                                                                    format='csv', fields=train_val_fields)\n",
        "ex = test_data[1]\n",
        "print(ex.prediction, ex.label)\n",
        "\n",
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "\n",
        "\n",
        "TEXT.build_vocab(train_data,  max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "ex = train_data[1]\n",
        "print(ex.prediction, ex.label)\n",
        "\n",
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
        "#print(LABEL.vocab.stoi)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, test_iterator = data.BucketIterator.splits(\n",
        "                                                    (train_data, test_data), \n",
        "                                                    batch_size = 1,\n",
        "                                                    sort_key=lambda x: len(x.prediction), # on what attribute the text should be sorted\n",
        "                                                    sort_within_batch = True,\n",
        "                                                    device = device)\n",
        "\n",
        "\n",
        "for b in test_iterator:\n",
        "  print(b.prediction)\n",
        "  break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbDjZUTgblsK"
      },
      "source": [
        "# HUMAN MODEL DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHGuxFQU-XoA"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "HUMAN_TEXT = data.Field(sequential=True, \n",
        "                tokenize='spacy', \n",
        "                tokenizer_language='en_core_web_sm',\n",
        "                include_lengths=True, \n",
        "                use_vocab=True)\n",
        "HUMAN_LABEL = data.Field(sequential=False, \n",
        "                  use_vocab=False, \n",
        "                  pad_token=None, \n",
        "                  unk_token=None,\n",
        "                  )\n",
        "HUMAN_IMID = data.Field(sequential=False,\n",
        "                  use_vocab=False,\n",
        "                  pad_token=None,\n",
        "                  unk_token=None,\n",
        "                  )\n",
        "\n",
        "\n",
        "\n",
        "human_train_val_fields = [\n",
        "    ('prediction', HUMAN_TEXT), # process it as text\n",
        "    ('label', HUMAN_LABEL), # process it as label\n",
        "    ('imid', HUMAN_IMID)\n",
        "]\n",
        "\n",
        "human_train_data, human_test_data = torchtext.legacy.data.TabularDataset.splits(path='/bias_data_for_ig/LSTM/nic/',train='train_nic_human.csv', test='test_nic_human.csv',\n",
        "                                                                    format='csv', fields=human_train_val_fields)\n",
        "\n",
        "\n",
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "\n",
        "HUMAN_TEXT.build_vocab(human_train_data,  max_size = MAX_VOCAB_SIZE)\n",
        "HUMAN_LABEL.build_vocab(human_train_data)\n",
        "\n",
        "for i in range(5):\n",
        "  ex = human_train_data[i]\n",
        "  print(ex.prediction, ex.label)\n",
        "\n",
        "print(f\"Unique tokens in TEXT vocabulary: {len(HUMAN_TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(HUMAN_LABEL.vocab)}\")\n",
        "#print(LABEL.vocab.stoi)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "human_train_iterator, human_test_iterator = data.BucketIterator.splits(\n",
        "                                                    (human_train_data, human_test_data), \n",
        "                                                    batch_size = 1,\n",
        "                                                    sort_key=lambda x: len(x.prediction), # on what attribute the text should be sorted\n",
        "                                                    sort_within_batch = True,\n",
        "                                                    device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0coLEQks6Zxg"
      },
      "source": [
        "# CAPTION MODEL "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdmtPvox_6Ru",
        "outputId": "187fd423-40f8-4abc-e29f-b3662a9f6fc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(722, 100, padding_idx=1)\n",
              "  (rnn): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "print(PAD_IDX)\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "model.load_state_dict(torch.load('/bias_data_for_ig/LSTM/nic/model_nic_model.pt', map_location=torch.device('cpu')))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkIoyVMj6c4t"
      },
      "source": [
        "# HUMAN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOMdrJVy6cV4",
        "outputId": "ee9bf73e-f0c0-4821-a287-18f1ffdf284a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(700, 100, padding_idx=1)\n",
              "  (rnn): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INPUT_DIM = len(HUMAN_TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = HUMAN_TEXT.vocab.stoi[HUMAN_TEXT.pad_token]\n",
        "print(PAD_IDX)\n",
        "\n",
        "model_human = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "model_human.load_state_dict(torch.load('/bias_data_for_ig/LSTM/nic/model_nic_human.pt', map_location=torch.device('cpu')))\n",
        "model_human.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZG3IIJIXb-8V"
      },
      "outputs": [],
      "source": [
        "def forward_with_sigmoid(input, text_lengths=None):\n",
        "    return torch.sigmoid(model(input, text_lengths=text_lengths)[0])\n",
        "\n",
        "def forward_with_sigmoid_human(input, text_lengths=None):\n",
        "    return torch.sigmoid(model_human(input, text_lengths=text_lengths)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLHHUBzHcbyX",
        "outputId": "14130f74-2bd3-412c-e50c-06e6897463c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "<unk>\n",
            "<captum.attr._models.base.TokenReferenceBase object at 0x7f72b8703400>\n"
          ]
        }
      ],
      "source": [
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "# itos is id -> token\n",
        "# stoi is token -> id\n",
        "print(TEXT.vocab.stoi['  '])\n",
        "print(TEXT.vocab.itos[UNK_IDX])\n",
        "\n",
        "# PADDING NEUTRAL\n",
        "token_reference = TokenReferenceBase(reference_token_idx=PAD_IDX)\n",
        "\n",
        "print(token_reference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR_DHdsh6xF4",
        "outputId": "6d6399da-ffad-416d-b917-a9f94bd592e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "<unk>\n",
            "<captum.attr._models.base.TokenReferenceBase object at 0x7f737ef3dfd0>\n"
          ]
        }
      ],
      "source": [
        "PAD_IDX = HUMAN_TEXT.vocab.stoi[HUMAN_TEXT.pad_token]\n",
        "UNK_IDX = HUMAN_TEXT.vocab.stoi[HUMAN_TEXT.unk_token]\n",
        "\n",
        "# itos is id -> token\n",
        "# stoi is token -> id\n",
        "print(HUMAN_TEXT.vocab.stoi['  '])\n",
        "print(HUMAN_TEXT.vocab.itos[UNK_IDX])\n",
        "\n",
        "# PADDING NEUTRAL\n",
        "token_reference = TokenReferenceBase(reference_token_idx=PAD_IDX)\n",
        "\n",
        "print(token_reference)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UWFrCNEAP51"
      },
      "source": [
        "# GRADIENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CWAc1ZLKAOsx"
      },
      "outputs": [],
      "source": [
        "lig = LayerIntegratedGradients(forward_with_sigmoid, model.embedding)\n",
        "lig_human = LayerIntegratedGradients(forward_with_sigmoid_human, model_human.embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zJ5y4WXfAW9A"
      },
      "outputs": [],
      "source": [
        "\n",
        "def interpret_sentence(model, sentence, text_lengths, vis_data_records, model_text, model_labels, lig, min_len = 15, label = 0, att_dict = {}):\n",
        "    text = [model_text.vocab.itos[t] for t in sentence.permute(1,0)]\n",
        "\n",
        "    indexed = [model_text.vocab.stoi[t] for t in text]\n",
        "    model.zero_grad()\n",
        "    input_indices = torch.tensor(indexed, device=device)\n",
        "    input_indices = input_indices.unsqueeze(0)\n",
        "\n",
        "    input_indices = sentence\n",
        "\n",
        "    # logit \n",
        "    pred = model(input_indices, text_lengths)[0]\n",
        "    print(input_indices, text_lengths)\n",
        "\n",
        "    # logit to prob\n",
        "    probs = torch.sigmoid(pred).item()\n",
        "    pred_ind = round(probs)\n",
        "\n",
        "    # generate reference indices for each sample\n",
        "    reference_indices = token_reference.generate_reference(text_lengths, device=device).unsqueeze(0)\n",
        "    # compute attributions and approximation delta using layer integrated gradients\n",
        "    attributions_ig, delta = lig.attribute(input_indices, reference_indices, \\\n",
        "                                           additional_forward_args=text_lengths,\n",
        "                                           n_steps=500, return_convergence_delta=True)\n",
        "\n",
        "    add_attributions_to_visualizer(attributions_ig, text, probs, pred_ind, label, delta, vis_data_records, model_labels, att_dict)\n",
        "\n",
        "    return vis_data_records\n",
        "\n",
        "def add_attributions_to_visualizer(attributions, text, pred, pred_ind, label, delta, vis_data_records, model_label, att_dict):\n",
        "    attributions = attributions.sum(dim=2).squeeze(0)\n",
        "    attributions = attributions / torch.norm(attributions)\n",
        "    attributions = attributions.cpu().detach().numpy()\n",
        "    \n",
        "  \n",
        "    model_pred = int(model_label.vocab.itos[pred_ind])\n",
        "    target = int(model_label.vocab.itos[label])\n",
        "    print(text)\n",
        "    if model_pred == target:\n",
        "      if target == 0:\n",
        "        att_dict['male_att_score'].append(attributions.sum())\n",
        "      elif target == 1:\n",
        "        att_dict['female_att_score'].append(attributions.sum())\n",
        "      \n",
        "    \n",
        "\n",
        "\n",
        "    # storing couple samples in an array for visualization purposes\n",
        "    vis_data_records.append(visualization.VisualizationDataRecord(\n",
        "                            attributions,\n",
        "                            pred,\n",
        "                            model_label.vocab.itos[pred_ind],\n",
        "                            model_label.vocab.itos[label],\n",
        "                            model_label.vocab.itos[0],\n",
        "                            attributions.sum(),\n",
        "                            text,\n",
        "                            delta))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "RQdTSHD5dEzt"
      },
      "outputs": [],
      "source": [
        "from tkinter.constants import E\n",
        "model_dict = {}\n",
        "human_dict = {}\n",
        "attributions_dict_model = {\n",
        "   'female_att_score':[],\n",
        "   'male_att_score':[],\n",
        "}\n",
        "attributions_dict_human = {\n",
        "    'female_att_score':[],\n",
        "    'male_att_score':[],\n",
        "}\n",
        "\n",
        "for b in human_test_iterator:\n",
        "  human_dict[b.imid.item()] = [b.prediction, b.label]\n",
        "\n",
        "for b in test_iterator:\n",
        "  model_dict[b.imid.item()] = [b.prediction, b.label]\n",
        "\n",
        "\n",
        "def run_sentence_interpretation(num_of_sentences):\n",
        "  # accumalate couple samples in this array for visualization purposes\n",
        "  human_data_records = []\n",
        "  model_data_records = []\n",
        "  \n",
        "  counter = 0\n",
        "\n",
        "  len_model_dict = len(model_dict)\n",
        "  len_human_dict = len(human_dict)\n",
        "\n",
        "  if len_model_dict <= len_human_dict:\n",
        "    bigger_dict = human_dict\n",
        "    smaller_dict = model_dict\n",
        "  else:\n",
        "    bigger_dict = model_dict\n",
        "    smaller_dict = human_dict\n",
        "\n",
        "  keys = bigger_dict.keys()\n",
        "\n",
        "  if num_of_sentences == 0:\n",
        "    num_of_sentences = len(keys)\n",
        "    print('Iterating over:', num_of_sentences, 'samples.')\n",
        "\n",
        "  for key in keys:      \n",
        "    if key not in smaller_dict:\n",
        "      continue\n",
        "\n",
        "    if counter % 30 == 0:\n",
        "      print('Currently at the ' + str(counter) + 'th image')\n",
        "    counter += 1\n",
        "\n",
        "\n",
        "    model_text, model_text_length = model_dict[key][0]\n",
        "    human_text, human_text_length = human_dict[key][0]\n",
        "    count = 0\n",
        "    human_text2 = []\n",
        "    for id in human_text:\n",
        "      if id.item() == 6:\n",
        "        count += 1\n",
        "        human_text2.append(HUMAN_TEXT.vocab.stoi['[MASK]'])\n",
        "      else:\n",
        "        human_text2.append(id)\n",
        "    human_text2 = torch.tensor([human_text2])\n",
        "    human_text_length2 = human_text_length #- count\n",
        "\n",
        "    human_text = human_text.permute(1,0)\n",
        "\n",
        "    human_label = human_dict[key][1]\n",
        "\n",
        "    interpret_sentence(model_human, human_text, human_text_length, model_text=HUMAN_TEXT, model_labels=HUMAN_LABEL, lig=lig_human, vis_data_records=human_data_records, label=human_label, att_dict=attributions_dict_human)\n",
        "  \n",
        "    model_text = model_text.permute(1,0)\n",
        "    model_label = model_dict[key][1]\n",
        "    \n",
        "\n",
        "    \n",
        "    interpret_sentence(model, model_text, model_text_length, model_text=TEXT ,model_labels=LABEL, lig=lig, vis_data_records=model_data_records, att_dict=attributions_dict_model, label=model_label)\n",
        "\n",
        "    if counter == num_of_sentences:\n",
        "      break\n",
        "\n",
        "  return human_data_records, model_data_records, attributions_dict_human, attributions_dict_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUalgrkLpWIi"
      },
      "outputs": [],
      "source": [
        "print('Visualize attributions based on Integrated Gradients for the human captions')\n",
        "human_records, model_records, _, _ = run_sentence_interpretation(0)\n",
        "_ = visualization.visualize_text(human_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGA3gTtEd_yV"
      },
      "outputs": [],
      "source": [
        "print('Visualize attributions based on Integrated Gradients for the model captions')\n",
        "_ = visualization.visualize_text(model_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HEMxnPn7HxfQ"
      },
      "outputs": [],
      "source": [
        "fem_sums = {\n",
        "    'nic': [],\n",
        "    'nic_plus': [],\n",
        "    'nic_equalizer': []\n",
        "}\n",
        "\n",
        "male_sums = {\n",
        "    'nic': [],\n",
        "    'nic_plus': [],\n",
        "    'nic_equalizer': []\n",
        "}\n",
        "\n",
        "for model_name in ['nic', 'nic_plus', 'nic_equalizer']:\n",
        "  for seed in [0, 12, 100, 200, 300, 400, 456, 500, 789, 1234]:\n",
        "\n",
        "    with open('/attributions/'+ model_name + '_seed' + str(seed) + 'attributions.pickle', 'rb') as handle:\n",
        "      current_model = pickle.load(handle)\n",
        "\n",
        "    fem_sums[model_name].append(np.mean(current_model['female_att_score']))\n",
        "    male_sums[model_name].append(np.mean(current_model['male_att_score']))\n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_JnAx7SQvjA"
      },
      "outputs": [],
      "source": [
        "for model_name in ['nic', 'nic_plus', 'nic_equalizer']:\n",
        "  fem_mean = np.mean(fem_sums[model_name])\n",
        "  fem_var = np.var(fem_sums[model_name])\n",
        "\n",
        "  print('FOR MODEL:', model_name)\n",
        "\n",
        "  print('Female mean:', fem_mean)\n",
        "  print('Female variance:', fem_var)\n",
        "  print('')\n",
        "\n",
        "  male_mean = np.mean(male_sums[model_name])\n",
        "  male_var = np.var(male_sums[model_name])\n",
        "\n",
        "  print('Male mean:', male_mean)\n",
        "  print('Male variance:', male_var)\n",
        "  print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX5PTHa3rqNu"
      },
      "source": [
        "## Genderword replacement analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xs8-MTtS30C"
      },
      "outputs": [],
      "source": [
        "correct_genderword = 0\n",
        "correct_masked = 0\n",
        "different = 0\n",
        "total = 0\n",
        "equal = 0\n",
        "LICd = 0 \n",
        "LICd2 = 0\n",
        "female_scores = 0\n",
        "male_scores = 0\n",
        "female_scores2 = 0\n",
        "male_scores2 = 0\n",
        "female_att_sum = 0\n",
        "male_att_sum = 0\n",
        "\n",
        "\n",
        "def create_text(text):\n",
        "    count = 0\n",
        "    text2 = []\n",
        "    for id in text:\n",
        "      if id.item() == 6:\n",
        "        count += 1\n",
        "        text2.append(HUMAN_TEXT.vocab.stoi['[MASK]'])\n",
        "      else:\n",
        "        text2.append(id)\n",
        "    text2 = torch.tensor([text2])\n",
        "    text_length2 = text_length\n",
        "    text = text.permute(1,0)\n",
        "    return text, text2\n",
        "\n",
        "def get_scores(text):\n",
        "    spred = torch.sigmoid(model(text, text_length)[0])\n",
        "    #spred2 = torch.sigmoid(model(text2, text_length)[0])\n",
        "    pred_gender = (spred >= 0.5000).int()\n",
        "    #pred_mask = (spred2 >= 0.5000).int()\n",
        "    female_score = spred\n",
        "    male_score = 1 - female_score\n",
        "    if male_score >= female_score:\n",
        "        pred_score = male_score\n",
        "    else:\n",
        "        pred_score = female_score\n",
        "    return spred, pred_gender, female_score, male_score, pred_score\n",
        "\n",
        "for i, batch in enumerate(human_test_iterator):  \n",
        "    text, text_length = batch.prediction\n",
        "    text, text2 = create_text(text)\n",
        "    label = batch.label\n",
        "    total += 1\n",
        "\n",
        "    spred, pred_gender, female_score, male_score, pred_score = get_scores(text)\n",
        "    spred2, pred_gender2, female_score2, male_score2, pred_score2 = get_scores(text2)\n",
        "\n",
        "    if (pred_gender != pred_gender2):\n",
        "      different += 1\n",
        "\n",
        "    # normal \n",
        "    if pred_gender == label:\n",
        "      correct_genderword += 1\n",
        "      LICd += pred_score\n",
        "      female_scores += female_score\n",
        "      male_scores += male_score\n",
        "\n",
        "    if pred_gender2 == label:\n",
        "      correct_masked += 1\n",
        "      LICd2 += pred_score2\n",
        "      female_scores2 += female_score2\n",
        "      male_scores2 += male_score2\n",
        "\n",
        "    \n",
        "\n",
        "print('--')\n",
        "#rint(\"number of different labels predicted\", different)\n",
        "#print(\"number of same labels predicted\", equal)\n",
        "print(\"LICd\", LICd/total)\n",
        "print(\"LICd2\", LICd2/total)\n",
        "print(\"female confidence\", female_scores.item())\n",
        "print(\"female confidence masked\", female_scores2.item())\n",
        "print(\"male confidence\", male_scores.item())\n",
        "print(\"male confidence masked\", male_scores2.item())\n",
        "print(total)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.15 (default, Nov 24 2022, 12:02:37) \n[Clang 14.0.6 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "9316ef86bac78c18b9c1bc4ec5daae637ae450971d4cbcf54996cc7257208990"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
